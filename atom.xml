<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[My Octopress Blog]]></title>
  <link href="http://sangha123.github.io/atom.xml" rel="self"/>
  <link href="http://sangha123.github.io/"/>
  <updated>2015-01-16T14:55:35-08:00</updated>
  <id>http://sangha123.github.io/</id>
  <author>
    <name><![CDATA[Sanghamitra Deb]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setting up an IPython Notebook on a Cluster]]></title>
    <link href="http://sangha123.github.io/blog/2015/01/14/setting-up-an-ipython-notebook-on-a-cluster/"/>
    <updated>2015-01-14T22:20:04-08:00</updated>
    <id>http://sangha123.github.io/blog/2015/01/14/setting-up-an-ipython-notebook-on-a-cluster</id>
    <content type="html"><![CDATA[<h1>Installing ipython notebook on a remote machine.</h1>

<p>IPython notebook is a good interface while working on a cluster remotely. You get the same experience as working on a local machine yet you can use the full power of your remote computer. In order to install ipython notebook you need Python 2.7 or higher. If you are using a red hat linux, python 2.6 is the default system, you will have install Python 2.7 separately.</p>

<p>In this blog post I am assuming regular ipython is installed (pip install ipython[all]).</p>

<p>1) The first step is to create a ipython profile on the server.</p>

<p>$ipython profile create nbserver</p>

<p>[ProfileCreate] Generating default config file: u&#8217;/home/uname/.config/ipython/profile_nbserver/ipython_config.py&#8217;</p>

<p>2) The next step is to create a certificate on the server.
In order to generate a certificate you need to create a certificate file. I keep my password in the same directory as</p>

<p>$cd home/uname/.config/ipython/profile_nbserver/.
$openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mycert.pem -out mycert.pem
This will create a certificate.
It is not necessary but I usually create a password as well to protect the notebook environment by running this small script.</p>

<h4>/home/uname/.config/ipython/profile_nbserver/mycert.pem</h4>

<p>Copy the location of mycert.pem</p>

<p>$python -c &ldquo;import IPython;print IPython.lib.passwd()&rdquo;
Enter password:
Verify password:</p>

<h4>sha1:xxx</h4>

<p>Copy the encrypted password.</p>

<p>Now open ipython_config.py with favorite editor &amp; put the path to mycert.pem &amp; provide the encrypted password.</p>

<p>$emacs -nw ipython_config.py
c.NotebookApp.certfile = u&#8217;/home/administrator/.config/ipython/profile_nbserver/mycert.pem&#8217;
c.NotebookApp.certfile = u&#8217;*&lsquo;
c.NotebookApp.password = sha1:xxx</p>

<p>Put ipython notebook on a fixed port.</p>

<p>c.NotebookApp.port = 9999</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Installing spark 1.0.0 on Pivotal Hadoop with Yarn client]]></title>
    <link href="http://sangha123.github.io/blog/2014/07/06/thoughts-from-spark-summit-2014/"/>
    <updated>2014-07-06T21:52:24-07:00</updated>
    <id>http://sangha123.github.io/blog/2014/07/06/thoughts-from-spark-summit-2014</id>
    <content type="html"><![CDATA[<p>In order to install spark on a yarn client it is not necessary to install it on all the worker nodes. In yarn mode the spark shell or pyspark is packaged into the assembly and sent to the executers.</p>

<h2>Creating the Spark Shell.</h2>

<p>In this particular machine I had hadoop-2.0.5_alpha installed. In order to have the correct hadoop &amp; yarn libraries it is important to change provide the correct cluster address for node manager and rescourcemanager in yarn-site.xml and hdfs-site.xml. Then</p>

<p>cd spark/
cp conf/spark-env.sh.template  conf/spark-env.sh</p>

<p>Now open spark-env.sh in your favorite editor and add
export YARN_CONF_DIR =&ldquo;path to/yarn-site.xml&rdquo;</p>

<p>Now to make sure that the spark jars are created with the right version of hadoop we need include the SPARK_YARN and SPARK_HADOOP_VERSION variables while building the spark assembly.</p>

<p>SPARK_HADOOP_VERSION=2.0.5_alpha SPARK_YARN=true sbt/sbt clean assembly</p>

<p>This builds the spark-shell. All the jars are saved in ./assembly/target/scala-2.10/</p>

<h2>Running an example</h2>

<p>bin/spark-submit &mdash;class org.apache.spark.examples.SparkPi &mdash;master yarn-client examples/target/scala-2.10/spark-examples-1.0.0-hadoop2.0.5-alpha.jar</p>

<p>You can check if the yarn client is running in the right port in your web browser while the code runs.</p>

<p>The spark-shell is opened using</p>

<p>bin/spark-shell &mdash;master yarn-client</p>

<p>You can regular spark:scala commands here</p>

<h2>Pyspark on Yarn.</h2>

<p>The above process does not load Pyspark on yarn. Pyspark needs to be build with maven and java 6 and not on redhat cluster.</p>

<p>Install Maven using sudo apt-get or pip.
In the bashrc</p>

<p>export MAVEN_OPTS=&ldquo;-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m&rdquo;
export PATH=$PATH:/path/to/maven</p>

<p>Next install java 6 compiler (the jar file compiled with java7 is not recognized).</p>

<p>Build Spark jar</p>

<p>cd /path/to/spark
JAVA_HOME=/path/to/java6_home ./make-distribution.sh &mdash;with-yarn &mdash;hadoop 2.0.5-alpha
cd assembly/target/scala_2.10</p>

<p>if your system is redhat,
scp <JAR> <cluster></p>

<p>Place the jar file in the /assembly/target/scala-2.10/ directory.</p>

<p>next from the spark directory in your redhat cluster and launch</p>

<p>bin/pyspark &mdash;master yarn-client</p>

<p>and code away &hellip;</p>
]]></content>
  </entry>
  
</feed>
